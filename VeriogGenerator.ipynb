{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea722e4",
   "metadata": {},
   "source": [
    "# Code Snippet for Verilog Code Generation \n",
    "#### -By Aymen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ceedad-8860-4a2d-8d11-8ab689f04ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(linewidth=400)\n",
    "from utils.matmul_optimization import read_index_array\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd563fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_one_hot(matrix):\n",
    "    return np.all(np.sum(matrix, axis=1) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03605447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_operation_matrix(matrix, matrix_binarized = False, bit_size = 8):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        matrix: given m x n matrix\n",
    "        bit_size: bit width of each element in the matrix\n",
    "        matrix_binarized: flag to show whether matrix is already binarized or not\n",
    "        \n",
    "    Output:\n",
    "        index_array: format of array is: [left#, right#, target#, 'operation'] \n",
    "            and first row is: [rows, cols, bit, 'parameters']\n",
    "    \"\"\"\n",
    "    # bit_size = bit_size+1\n",
    "    rows1, cols1 = matrix.shape \n",
    "    bin_max_length = cols1*bit_size*3\n",
    "    bin_weight = cols1*bit_size\n",
    "    \n",
    "    # Binarize the matrix\n",
    "    if not matrix_binarized:\n",
    "        bin_matrix_ext = np.zeros( (rows1, bin_max_length), dtype=np.uint8)\n",
    "        # Element wise conversion of each element to binary number for 2^bit notation\n",
    "        bin_matrix = ((matrix.reshape(-1, 1) & (2**np.arange(bit_size)[::-1])) != 0).astype(int)\n",
    "        bin_matrix = bin_matrix.reshape(rows1, -1)\n",
    "        # Copying elements of bin matrix to extended binarized matrix\n",
    "        bin_matrix_ext[:,:bit_size*cols1] = bin_matrix\n",
    "    else:\n",
    "        bin_matrix_ext = matrix\n",
    "    \n",
    "    # Start of the algorithm proper , we find the max number of matching one.\n",
    "    # find the number of max ones of input binary matrix\n",
    "    max_matching = bin_matrix_ext.sum(axis=0).max()\n",
    "\n",
    "    \n",
    "    # process binary matrix\n",
    "    rows2, cols2 = bin_matrix_ext.shape\n",
    "    cnt = 0 #count number of matches\n",
    "    add_count = [0]*rows2 #count number of additions \n",
    "    one_count = int(max_matching) # count number of one in column\n",
    "\n",
    "    # Initialize index_array\n",
    "    # the first row of index_array is to store required parameters, [rows, cols, bit_size, 'parameters']\n",
    "    index_array = []\n",
    "    index_array.append([rows1, cols1, bit_size,'parameters'])\n",
    "    a = 1\n",
    "    for i in range(bin_weight):\n",
    "        if((i%bit_size)==0):\n",
    "            #print(\"MEM[%d] = -(input_vector[%d] << %d);\" %(i,a-1,a*bit_size-i-1))\n",
    "            index_array.append([i,a-1,a*bit_size-i-1,'-<<'])\n",
    "        else:\n",
    "            #print(\"MEM[%d] = input_vector[%d] << %d;\" %(i,a-1,a*bit_size-i-1))\n",
    "            index_array.append([i,a-1,a*bit_size-i-1,'<<'])\n",
    "        if((i+1)%bit_size==0):\n",
    "            a+=1\n",
    "    \n",
    "    # While there is a column with more than one '1' in binary_expanded_matrix:\n",
    "    while(one_count >= 1):\n",
    "        #print(\"\\n Max number of 1: %d\" % one_count)\n",
    "        i = 0\n",
    "        # while(i < cols2 and bin_max_length > bin_weight+cnt):\n",
    "        while(i < bin_matrix_ext.shape[1]): # and bin_max_length > bin_weight+cnt):\n",
    "            # doubling the matrix size whenever it runs out of memory  \n",
    "            if bin_max_length <= bin_weight+cnt+2: \n",
    "                print('Increasing matrix size, current bin_max_length:', bin_max_length, 'bin_weight:', bin_weight, 'cnt:', cnt)\n",
    "                bin_max_length += bin_matrix_ext.shape[1]\n",
    "                bin_matrix_ext = np.hstack((bin_matrix_ext, np.zeros((rows1, bin_matrix_ext.shape[1]), \n",
    "                                                            dtype=np.int32)))\n",
    "                print('Increasing matrix size, updated bin_max_length:', bin_max_length)\n",
    "            \n",
    "            # print('i:', i, 'searching for col:', bin_matrix_ext[:,i])\n",
    "            # print('one_count:', one_count, 'i:', i)\n",
    "            if((np.sum(bin_matrix_ext[:,i]==1))>=one_count):\n",
    "                matching_idx = ((bin_matrix_ext[:,i] @ bin_matrix_ext) == one_count).nonzero()[0]\n",
    "                # print('matching_idx:', matching_idx)\n",
    "                if len(matching_idx) > 1:\n",
    "                    j = matching_idx[0] if i != matching_idx[0] else matching_idx[1]\n",
    "                    # print('j:', j, 'found col:', bin_matrix_ext[:,j])\n",
    "                    #print(\"MEM[%d] = MEM[%d] + MEM[%d];\"%(bin_weight+cnt,i,j))\n",
    "                    index_array.append([i,j,bin_weight+cnt,'+'])\n",
    "\n",
    "                    bin_matrix_ext[:,bin_weight+cnt] = bin_matrix_ext[:,i]&bin_matrix_ext[:,j]\n",
    "                    bin_matrix_ext[:,i] = bin_matrix_ext[:,i]&(~bin_matrix_ext[:,bin_weight+cnt])\n",
    "                    bin_matrix_ext[:,j] = bin_matrix_ext[:,j]&(~bin_matrix_ext[:,bin_weight+cnt])\n",
    "\n",
    "\n",
    "                    cnt += 1\n",
    "                    add_count[rows2-one_count] += 1\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        if check_one_hot(bin_matrix_ext):\n",
    "            break\n",
    "        #print(\"Counts of Additions for %d one match:  %d\" %(one_count,add_count[rows2-one_count]))\n",
    "        one_count=one_count-1\n",
    "    \n",
    "    # Adding non zero operations at the end\n",
    "    nonzero_row_cols = bin_matrix_ext.nonzero() \n",
    "    nonzero_rows = nonzero_row_cols[0]\n",
    "    nonzero_cols = nonzero_row_cols[1]\n",
    "    index_array = index_array + [[row, col, -1, '='] for row, col in zip(nonzero_rows, nonzero_cols)]\n",
    "    \n",
    "    return index_array, bin_matrix_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c98ac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_ array contient deja N_rows et N_columns, et n'a pas beoins de n_bits_weights, mais on a besoin de n_bits_input pour la structure du module \n",
    "def generate_verilog_optimized(index_array, input_bitwidth = 6, filename = ''):\n",
    "    # define parameters\n",
    "    rows1, cols1 = index_array[0][0], index_array[0][1]\n",
    "    weight_bitwidth = index_array[0][2]\n",
    "    output_bitwidth = weight_bitwidth + input_bitwidth + math.ceil(math.log(cols1 - 1 ,2))\n",
    "    mem_size = max(row[2] for row in index_array)\n",
    "    \n",
    "    added_statements = \"\"\n",
    "    for row in index_array[1:]:\n",
    "        if row[2] != -1:\n",
    "            if row[3] == '<<':\n",
    "                added_statements += f\"assign MEM[{row[0]}] = input_vector[{row[1]}] {row[3]} {row[2]};\\n\"\n",
    "            elif row[3] == '-<<':\n",
    "                added_statements += f\"assign MEM[{row[0]}] = -(input_vector[{row[1]}] << {row[2]});\\n\"\n",
    "            # actually, we don't consider >> operation here.\n",
    "            elif row[3] == '>>':\n",
    "                added_statements += f\"assign MEM[{row[0]}] = input_vector[{row[1]}] {row[3]} {row[2]};\\n\"\n",
    "            elif row[3] == '+':\n",
    "                added_statements += f\"assign MEM[{row[2]}] = MEM[{row[0]}] {row[3]} MEM[{row[1]}];\\n\"\n",
    "        else:\n",
    "            added_statements += f\"assign output_vector[{row[0]}] = MEM[{row[1]}];\\n\"    \n",
    "    \n",
    "    statements = [\n",
    "        \"// Verilog module\\n\",\n",
    "        f\"module matrix_mult_optimized_{rows1}x{cols1}_{weight_bitwidth}_{mem_size}#(\\n\",\n",
    "        f\"    parameter ROWS = {rows1},\\n\",\n",
    "        f\"    parameter COLS = {cols1},\\n\",\n",
    "        f\"    parameter MEM_SIZE = {mem_size},\\n\",\n",
    "        f\"    parameter input_bit_width = {input_bitwidth},\\n\",\n",
    "        f\"    parameter output_bit_width = {output_bitwidth}\\n\",\n",
    "        \")(\\n\",\n",
    "        \"    input wire signed [input_bit_width-1:0] input_vector [0: COLS-1],\\n\",\n",
    "        f\"   output wire signed [output_bit_width-1:0] output_vector [0: ROWS-1]\\n\",\n",
    "        \");\\n\\n\",\n",
    "        f\"wire signed [output_bit_width-1:0] MEM [0:MEM_SIZE];\\n\\n\",\n",
    "        added_statements,\n",
    "        \"\\n\",\n",
    "        \"endmodule\\n\"\n",
    "    ]\n",
    "    # Create and open a new Verilog file\n",
    "    # verilog_file = open(f\"output/matrix_multiplier_{mem_size}.sv\", \"w\")\n",
    "    verilog_file = open(f\"Verilog_Files/matrix_mult_optimized_{filename}{rows1}x{cols1}_{weight_bitwidth}_{mem_size}.sv\", \"w\")\n",
    "\n",
    "    # Write the strings to the Verilog file\n",
    "    for statement in statements:\n",
    "        verilog_file.write(statement)\n",
    "    # Close the Verilog file\n",
    "    verilog_file.close()\n",
    "    print(\"Successfully generate verilog file!\")\n",
    "    return verilog_file, weight_bitwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a297475",
   "metadata": {},
   "source": [
    "##### Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9819d87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import brevitas.nn as qnn\n",
    "\n",
    "# Define the transformation for dataset (if needed)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.quant_inp1 = qnn.QuantIdentity(bit_width=8, signed=False, return_quant_tensor=True)\n",
    "        self.fc1 = qnn.QuantLinear(784, 64, bias=False,\n",
    "                                   weight_bit_width=8,\n",
    "                                   bias_bit_width=8)\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=26, return_quant_tensor=True)\n",
    "        self.quant_inp2 = qnn.QuantIdentity(bit_width=8, signed=False, return_quant_tensor=True)\n",
    "        self.fc2 = qnn.QuantLinear(64, 64, bias=False,\n",
    "                                   weight_bit_width=8,\n",
    "                                   bias_bit_width=8)\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=22, return_quant_tensor=True)\n",
    "        self.quant_inp3 = qnn.QuantIdentity(bit_width=8, signed=False, return_quant_tensor=True)\n",
    "        self.fc3 = qnn.QuantLinear(64, 64, bias=False,\n",
    "                                   weight_bit_width=8,\n",
    "                                   bias_bit_width=8)\n",
    "        self.relu3 = qnn.QuantReLU(bit_width=22, return_quant_tensor=True)\n",
    "        self.quant_inp4 = qnn.QuantIdentity(bit_width=8, signed=False, return_quant_tensor=True)\n",
    "        self.fc4 = qnn.QuantLinear(64, 10, bias=False,\n",
    "                                   weight_bit_width=8,\n",
    "                                   bias_bit_width=8)\n",
    "        self.relu4 = qnn.QuantReLU(bit_width=22, return_quant_tensor=True)\n",
    "        self.quant_inp5 = qnn.QuantIdentity(bit_width=8, signed=False, return_quant_tensor=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)  # Flatten the input tensor\n",
    "        x = self.quant_inp1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.quant_inp2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.quant_inp3(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.quant_inp4(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.quant_inp5(x)\n",
    "        return x\n",
    "    \n",
    "    #new function to extract activations after each layer\n",
    "\n",
    "    def forward_compute(self, x):\n",
    "        x = torch.flatten(x, 1)  # Flatten the input tensor\n",
    "        y = []\n",
    "        z = []\n",
    "        x = self.quant_inp1(x)\n",
    "        y.append(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        y.append(x)\n",
    "        x = self.quant_inp2(x)\n",
    "        z.append(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        y.append(x)\n",
    "        x = self.quant_inp3(x)\n",
    "        z.append(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        y.append(x)\n",
    "        x = self.quant_inp4(x)\n",
    "        z.append(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        y.append(x)\n",
    "        x = self.quant_inp5(x)\n",
    "        z.append(x)\n",
    "        return x,y,z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eec99838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APA4Lea6nSC3hkmmc4SONSzMfQAdadc2dzZSmK7tpoJB/BKhU/kahoq1pupXuj6hDf6dcyW13CSY5YzhlyCDj8CRXXWvxe8b20SxtrH2lVOQbqCOU9OmWUn/APVXUfEfxBqCfDzSNJ16S2uNd1JxqEqpbohtIMYjQbQBlsEnv1HpXj9Fdx8OvDNlqNxe+IdeVh4e0VPPucLnznyNkQ/3j1/LjOa57xPr9z4n8R3usXQ2vcSZVB0jQcKo9gABWRRXSxeOdXt/A0nhGAW0WnSzGaZ1j/ey8g7SxOMZA6AHjrXNUV//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAzUlEQVR4AWNgGDaAEeKTkNRnP5a+uIPmLajkPQWg+OerUMknXWfALBYIP1X/mpahg8VjWQaGP68lGR5BJKE6wUoEDc+YMjD8uHVdKGca1AwMKvjvRSEMQaiA2Mv/wVAmE4aabNH3NzEEoQLWP//Z4ZJjaP23mxWXJOfZH1a45Bjq/m3DKef9+4MlLknhu/+W4ZJjPv3vtjIuSbV//3xxyck/+FeMHNYo6lr//TNBEUDi2H5Cl0QKWxsehrtfkBQzMEAjGyJ20fkdiiReDgBpETyQooNMkwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = datasets.MNIST(root='./data', train=False, download=True)\n",
    "data, target = test_data[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0867ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the FP32 model weights\n",
    "# model_fp32_weights = torch.load('model_fp32.pth')\n",
    "\n",
    "# Initialize the quantized model\n",
    "model = SimpleNN()\n",
    "\n",
    "# # Copy the weights from FP32 model to quantized model\n",
    "# model.fc1.weight.data = model_fp32_weights['fc1.weight'].clone().detach()\n",
    "# model.fc2.weight.data = model_fp32_weights['fc2.weight'].clone().detach()\n",
    "# model.fc3.weight.data = model_fp32_weights['fc3.weight'].clone().detach()\n",
    "# model.fc4.weight.data = model_fp32_weights['fc4.weight'].clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0581efed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_weights_quantized.pth', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c46aae",
   "metadata": {},
   "source": [
    "Defining some of the Helper Functions which we will need in further code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3358c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary(val, bitwidth):\n",
    "    if val < 0:\n",
    "        val = (1 << bitwidth) + val\n",
    "    return f\"{val:0{bitwidth}b}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24b270a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_int_n(values, scale, bit_width, signed=True):\n",
    "    # Calculate the scaled values\n",
    "    scaled_values = values / scale\n",
    "    scaled_values = np.round(scaled_values)\n",
    "    \n",
    "    if(signed == True):\n",
    "        # Determine the range for n-bit signed integers\n",
    "        int_n_min = -2**(bit_width - 1)\n",
    "        int_n_max = 2**(bit_width - 1) - 1\n",
    "        \n",
    "        # Clip the values to fit within n-bit signed integer range\n",
    "        scaled_values = np.clip(scaled_values, int_n_min, int_n_max)\n",
    "        \n",
    "        # Convert to n-bit integers by truncating to fit in n-bits\n",
    "        int_n_values = scaled_values.astype(np.int32)  # Initially convert to 32-bit integers\n",
    "        \n",
    "        # Manually handle the n-bit integer range\n",
    "        int_n_values = int_n_values & ((1 << bit_width) - 1)  # Mask to n bits\n",
    "        \n",
    "        # Handle sign extension for negative values\n",
    "        sign_bit = 1 << (bit_width - 1)\n",
    "        int_n_values = np.where(int_n_values & sign_bit, int_n_values | ~((1 << bit_width) - 1), int_n_values)\n",
    "        \n",
    "    else:\n",
    "        # Determine the range for n-bit unsigned integers\n",
    "        int_n_min = 0\n",
    "        int_n_max = 2**bit_width - 1\n",
    "\n",
    "        # Clip the values to fit within n-bit unsigned integer range\n",
    "        scaled_values = np.clip(scaled_values, int_n_min, int_n_max)\n",
    "\n",
    "        # Convert to n-bit integers by truncating to fit in n-bits\n",
    "        int_n_values = scaled_values.astype(np.uint32)  # Initially convert to 32-bit unsigned integers\n",
    "\n",
    "        # Manually handle the n-bit integer range\n",
    "        int_n_values = int_n_values & ((1 << bit_width) - 1)  # Mask to n bits\n",
    "        \n",
    "    \n",
    "    return int_n_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16768471",
   "metadata": {},
   "source": [
    "# Generating the Verilog Codes for different Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ede28675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "data, targets = train_dataset[0]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70283bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/myenv/lib/python3.10/site-packages/torch/_tensor.py:1419: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1921.)\n",
      "  return super().rename(names)\n"
     ]
    }
   ],
   "source": [
    "# re-evaluating the model for generating outputs\n",
    "with torch.no_grad():\n",
    "    out, outputs, quants = model.forward_compute(data)\n",
    "    _, predicted = torch.max(out.value, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5d54ea",
   "metadata": {},
   "source": [
    "Extracting integer Weights from Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "243c7036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generate verilog file!\n"
     ]
    }
   ],
   "source": [
    "# uncomment the code to generate the verilog codes\n",
    "\n",
    "\n",
    "bitwidths = 8\n",
    "layers = [0,1,2,3] #add the layers which you want to generate for\n",
    "\n",
    "count = 0\n",
    "\n",
    "# Assuming 'model' is already defined and loaded\n",
    "for layer in layers:\n",
    "    for name, layer in model.named_modules():\n",
    "        # Check if the layer is a Linear layer\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            \n",
    "            # Extract the weight matrix, move to CPU, detach from the computation graph, and convert to NumPy array\n",
    "            if(count == 3):\n",
    "                int_weights = layer.int_weight().detach().cpu().numpy()\n",
    "                \n",
    "                index_array, _ = minimize_operation_matrix(int_weights, bit_size = bitwidths)\n",
    "                verilog_code, _ = generate_verilog_optimized(index_array , bitwidths)\n",
    "                \n",
    "            count = count + 1\n",
    "            \n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b67f69f",
   "metadata": {},
   "source": [
    "TestBench Generation for Verification of Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3dd35196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000010001',\n",
       " '001011110',\n",
       " '000000000',\n",
       " '001001100',\n",
       " '011111100',\n",
       " '011101100',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000110100',\n",
       " '001010100',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011000001',\n",
       " '001011000',\n",
       " '011111000',\n",
       " '011100010',\n",
       " '010000101',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '011011010',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011110100',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '010110101',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '010001011',\n",
       " '001101100',\n",
       " '011101100',\n",
       " '011100000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000111000',\n",
       " '000000000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '010011001',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000110100',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000110100',\n",
       " '011111000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000010111',\n",
       " '011111000',\n",
       " '001111100',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '001111100',\n",
       " '011111000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '011100000',\n",
       " '011000001',\n",
       " '001000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '011011110',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '001110100',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '000101100',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '011110110',\n",
       " '011111000',\n",
       " '001110110',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '011110000',\n",
       " '011111000',\n",
       " '011110000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000101',\n",
       " '001101110',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '010011101',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000101001',\n",
       " '011001001',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011110010',\n",
       " '001101100',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '010111001',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '010010001',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '010101001',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '010001011',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '001010110',\n",
       " '010110101',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '010000101',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '001011000',\n",
       " '011000011',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011100110',\n",
       " '000001011',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000010001',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '011111000',\n",
       " '010100111',\n",
       " '000001111',\n",
       " '000001001',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000',\n",
       " '000000000']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming the output is the same as mentioned\n",
    "output_values = outputs[0].value.numpy().flatten()\n",
    "scale = outputs[0].scale.item()\n",
    "bitwidth = 9\n",
    "\n",
    "# Convert the floating point values to 8-bit signed integers\n",
    "input_vector = float_to_int_n(output_values, scale, bitwidth, False)\n",
    "\n",
    "# Print the input vector for Verilog\n",
    "input_vector_verilog = [to_binary(val, bitwidth) for val in input_vector]\n",
    "\n",
    "input_vector_verilog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85d89047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verilog testbench generated and saved as 'testbench.v'\n"
     ]
    }
   ],
   "source": [
    "# Generate the Verilog testbench code\n",
    "verilog_code = f\"\"\"\n",
    "`timescale 1ns / 1ps\n",
    "\n",
    "module Testbench#(\n",
    "    parameter ROWS = 64,\n",
    "    parameter COLS = 64,\n",
    "    parameter input_bit_width = {bitwidth},\n",
    "    parameter output_bit_width = {int(outputs[1].bit_width.item())}\n",
    ")();\n",
    "        \n",
    "    logic signed [input_bit_width-1:0] input_vector [0: COLS-1];\n",
    "    logic signed [output_bit_width-1:0] output_vector [0: ROWS-1];\n",
    "        \n",
    "    DNN_Layer test1(\n",
    "        .input_vector(input_vector),\n",
    "        .output_vector(output_vector)\n",
    "    );\n",
    "        \n",
    "    initial begin\n",
    "        // Initialize input_vector with the values from Python\n",
    "\"\"\"\n",
    "\n",
    "# Add the input vector initialization in Verilog format\n",
    "for i, val in enumerate(input_vector_verilog):\n",
    "    verilog_code += f\"        input_vector[{i}] = 9'b{val};\\n\"\n",
    "\n",
    "# Finish the Verilog code\n",
    "verilog_code += \"\"\"\n",
    "        // Wait for some time to observe the output\n",
    "        #10;\n",
    "\n",
    "        // Test the output against the expected values\n",
    "        // Example: assert(output_vector[0] == expected_value);\n",
    "        // Add your checks here\n",
    "\n",
    "        // Finish the simulation\n",
    "        $finish;\n",
    "    end\n",
    "endmodule\n",
    "\"\"\"\n",
    "\n",
    "# Write the Verilog code to a file\n",
    "with open(\"Verilog_Files/testbench.v\", \"w\") as file:\n",
    "    file.write(verilog_code)\n",
    "\n",
    "print(\"Verilog testbench generated and saved as 'testbench.v'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ac56c9",
   "metadata": {},
   "source": [
    "# Output Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d22c68c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0339, -0.0129, -0.0118,  ...,  0.0244, -0.0331,  0.0150],\n",
      "        [ 0.0200,  0.0345,  0.0050,  ..., -0.0131, -0.0158, -0.0032],\n",
      "        [-0.0068,  0.0080,  0.0316,  ...,  0.0113, -0.0013, -0.0168],\n",
      "        ...,\n",
      "        [-0.0328, -0.0090, -0.0313,  ..., -0.0213, -0.0153,  0.0156],\n",
      "        [-0.0254,  0.0106, -0.0115,  ..., -0.0043, -0.0261, -0.0339],\n",
      "        [-0.0116,  0.0351, -0.0144,  ...,  0.0027, -0.0039,  0.0100]],\n",
      "       requires_grad=True)\n",
      "Layer 0 - Integer Weights:\n",
      "[[ 6 -2 -2 ...  4 -6  3]\n",
      " [ 3  6  1 ... -2 -3 -1]\n",
      " [-1  1  5 ...  2  0 -3]\n",
      " ...\n",
      " [-6 -2 -5 ... -4 -3  3]\n",
      " [-4  2 -2 ... -1 -4 -6]\n",
      " [-2  6 -2 ...  0 -1  2]]\n",
      "Weights Shape: (64, 784)\n",
      "Weight Scale: 0.005887963343411684\n"
     ]
    }
   ],
   "source": [
    "# Extract and print integer weights and scales for specified layers\n",
    "layers = [0]\n",
    "count = 0\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, qnn.QuantLinear):\n",
    "        if count in layers:\n",
    "            int_weights = module.int_weight().detach().cpu().numpy()\n",
    "            weight_scale = module.quant_weight_scale().detach().cpu().numpy() if hasattr(module, 'quant_weight_scale') else None\n",
    "            print(module.weight)\n",
    "            print(f'Layer {count} - Integer Weights:')\n",
    "            print(int_weights)\n",
    "            print(f'Weights Shape: {int_weights.shape}')\n",
    "            print(f'Weight Scale: {weight_scale}')\n",
    "        \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb157ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[26804492., 13827634.,  8363090.,        0.,  5796438.,  4615285.,\n",
       "                0.,  8603659., 24962580., 12776851., 21841688., 15636567.,\n",
       "          2828695.,        0.,        0.,        0.,        0.,  9783727.,\n",
       "         16872168.,        0.,        0.,        0.,        0.,  5260201.,\n",
       "                0., 13795961.,        0.,        0.,        0., 12243000.,\n",
       "         26171288.,  6530076.,        0.,        0.,        0.,        0.,\n",
       "                0., 13628497.,        0.,        0.,        0.,  2786830.,\n",
       "         33396826.,   619320., 14350421., 11705895., 10250332.,        0.,\n",
       "                0.,  9115816., 10247077., 41936924.,        0.,  6464784.,\n",
       "                0.,        0.,        0.,  6642662., 23828068., 19791752.,\n",
       "          4429164.,        0.,  5322459.,        0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1]/ (outputs[1].scale.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "509c95ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([123566,  63744,  38553,      0,  26721,  21276,      0,  39662, 115075,  58900, 100688,  72083,  13040,      0,      0,      0,      0,  45102,  77779,      0,      0,      0,      0,  24249,      0,  63598,      0,      0,      0,  56439, 120647,  30103,      0,      0,      0,      0,      0,  62826,      0,      0,      0,  12847, 153956,   2855,  66154,  53963,  47253,      0,      0,\n",
       "        42023,  47238, 193325,      0,  29802,      0,      0,      0,  30622, 109845,  91238,  20418,      0,  24536,      0], dtype=uint32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming the output is the same as mentioned\n",
    "output_values = outputs[1].value.numpy().flatten()\n",
    "scale = outputs[0].scale.item()*weight_scale\n",
    "bitwidth = int(outputs[0].bit_width.item())\n",
    "\n",
    "# Convert the floating point values to 8-bit signed integers\n",
    "vec = float_to_int_n(output_values, scale, 26, False)\n",
    "\n",
    "vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9330f18",
   "metadata": {},
   "source": [
    "# Verilog Code Generation for Intermediate Scale Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f8b238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the weight scale of all the layers\n",
    "count = 0\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, qnn.QuantLinear):\n",
    "        if count == 0:\n",
    "            weight_scale = module.quant_weight_scale().detach().cpu().numpy() if hasattr(module, 'quant_weight_scale') else None\n",
    "        count +=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4970e4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1472.022871883505"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracting the values\n",
    "number = (quants[0].scale.item() * weight_scale)/ (outputs[1].scale.item())\n",
    "number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b51566d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_off(number, decimals=0):\n",
    "    \"\"\"\n",
    "    Rounds off a number to a specified number of decimal places.\n",
    "\n",
    "    Args:\n",
    "    number (float): The number to be rounded off.\n",
    "    decimals (int): The number of decimal places to round off to.\n",
    "\n",
    "    Returns:\n",
    "    float: The rounded off number.\n",
    "    \"\"\"\n",
    "    multiplier = 10 ** decimals\n",
    "    return round(number * multiplier) / multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f83d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_binary(num):\n",
    "    if num < 0:\n",
    "        raise ValueError(\"Only non-negative integers are supported.\")\n",
    "    return bin(num).replace(\"0b\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "800fe2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "number = int(round(number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34c75aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The binary representation of 1472 is 10111000000\n"
     ]
    }
   ],
   "source": [
    "binary_representation = int_to_binary(number)\n",
    "print(f\"The binary representation of {number} is {binary_representation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3dd82a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "verilog_code = '''\n",
    "\n",
    "mem[0] = in[i] << 5;\n",
    "mem[1] = in[i] << 4;\n",
    "mem[2] = in[i] << 3;\n",
    "mem[3] = in[i] << 2;\n",
    "mem[4] = in[i] << 1;\n",
    "mem[5] = mem[0] + mem[1];\n",
    "mem[6] = mem[2] + mem[3];\n",
    "mem[7] = mem[4] + mem[5];\n",
    "mem[8] = mem[6] + mem[7];\n",
    "\n",
    "\n",
    "''' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6017486",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c4a6a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1673479\n"
     ]
    }
   ],
   "source": [
    "# Extract and print integer weights and scales for specified layers\n",
    "layers = [0]\n",
    "count = 0\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, qnn.QuantLinear):\n",
    "        if count in layers:\n",
    "            int_weights = module.int_weight().detach().cpu().numpy()\n",
    "            absolute_values = np.abs(int_weights)\n",
    "            row_sums = np.sum(absolute_values, axis=1)\n",
    "            number = row_sums*127\n",
    "            max_value = np.max(number)\n",
    "            num = np.log2(max_value)\n",
    "            print(max_value)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62d16b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/myenv/lib/python3.10/site-packages/torch/_tensor.py:1419: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1921.)\n",
      "  return super().rename(names)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, targets \u001b[38;5;129;01min\u001b[39;00m test_dataset:\n\u001b[0;32m---> 12\u001b[0m         out, outputs, quants \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m         count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules():\n",
      "Cell \u001b[0;32mIn[8], line 83\u001b[0m, in \u001b[0;36mSimpleNN.forward_compute\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m z\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m     82\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n\u001b[0;32m---> 83\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m     85\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_inp4(x)\n",
      "File \u001b[0;32m/workspace/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/myenv/lib/python3.10/site-packages/brevitas/nn/quant_layer.py:141\u001b[0m, in \u001b[0;36mQuantNonLinearActLayer.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_global_is_quant_layer(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m--> 141\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpack_output(out)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/workspace/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/myenv/lib/python3.10/site-packages/brevitas/proxy/runtime_quant.py:152\u001b[0m, in \u001b[0;36mActQuantProxyFromInjector.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    150\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfused_activation_quant_proxy\u001b[38;5;241m.\u001b[39mactivation_impl(y)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfused_activation_quant_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# If y is an empty QuantTensor, we need to check if this is a passthrough proxy,\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# otherwise return an empty QuantTensor\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m f: f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, y)):\n",
      "File \u001b[0;32m/workspace/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/myenv/lib/python3.10/site-packages/brevitas/proxy/runtime_quant.py:82\u001b[0m, in \u001b[0;36mFusedActivationQuantProxy.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;129m@brevitas\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript_method\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     81\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_impl(x)\n\u001b[0;32m---> 82\u001b[0m     x, output_scale, output_zp, output_bit_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, output_scale, output_zp, output_bit_width\n",
      "File \u001b[0;32m/workspace/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/myenv/lib/python3.10/site-packages/brevitas/core/quant/int.py:152\u001b[0m, in \u001b[0;36mRescalingIntQuant.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;129m@brevitas\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript_method\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor, Tensor, Tensor]:\n\u001b[1;32m    151\u001b[0m     bit_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsb_clamp_bit_width_impl()\n\u001b[0;32m--> 152\u001b[0m     threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaling_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     int_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mint_scaling_impl(bit_width)\n\u001b[1;32m    154\u001b[0m     scale \u001b[38;5;241m=\u001b[39m threshold \u001b[38;5;241m/\u001b[39m int_threshold\n",
      "File \u001b[0;32m/workspace/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/myenv/lib/python3.10/site-packages/brevitas/core/scaling/standalone.py:356\u001b[0m, in \u001b[0;36mParameterFromRuntimeStatsScaling.forward\u001b[0;34m(self, stats_input)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m--> 356\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mabs_binary_sign_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp_scaling\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestrict_scaling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/workspace/myenv/lib/python3.10/site-packages/brevitas/function/ops_ste.py:370\u001b[0m, in \u001b[0;36mabs_binary_sign_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mabs(x)\n\u001b[0;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn_prefix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd_ste_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs_binary_sign_grad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/myenv/lib/python3.10/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/myenv/lib/python3.10/site-packages/brevitas/ops/autograd_ste_ops.py:372\u001b[0m, in \u001b[0;36mAbsBinarySignGradFn.forward\u001b[0;34m(ctx, x)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 372\u001b[0m     ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[43mbinary_sign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mint8))  \u001b[38;5;66;03m# save some memory\u001b[39;00m\n\u001b[1;32m    373\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(x)\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval() \n",
    "\n",
    "max_values_layer1 = []\n",
    "max_values_layer2 = []\n",
    "max_values_layer3 = []\n",
    "max_value = 0\n",
    "\n",
    "# re-evaluating the model for generating outputs\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_dataset:\n",
    "        out, outputs, quants = model.forward_compute(data)\n",
    "        count = 0\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, qnn.QuantLinear):\n",
    "                if count == 0:\n",
    "                    weight_scale = module.quant_weight_scale().detach().cpu().numpy() if hasattr(module, 'quant_weight_scale') else None\n",
    "                    result_tensor = outputs[1] / (outputs[0].scale.item() * weight_scale)\n",
    "                    result_array = result_tensor.numpy()\n",
    "                    max_value = np.max(result_array)\n",
    "                    max_values_layer1.append(max_value)\n",
    "                if count == 1:\n",
    "                    weight_scale = module.quant_weight_scale().detach().cpu().numpy() if hasattr(module, 'quant_weight_scale') else None\n",
    "                    result_tensor = outputs[2] / (quants[0].scale.item() * weight_scale)\n",
    "                    result_array = result_tensor.numpy()\n",
    "                    max_value = np.max(result_array)\n",
    "                    max_values_layer2.append(max_value)\n",
    "                if count == 2:\n",
    "                    weight_scale = module.quant_weight_scale().detach().cpu().numpy() if hasattr(module, 'quant_weight_scale') else None\n",
    "                    result_tensor = outputs[3] / (quants[1].scale.item() * weight_scale)\n",
    "                    result_array = result_tensor.numpy()\n",
    "                    max_value = np.max(result_array)\n",
    "                    max_values_layer3.append(max_value)\n",
    "                \n",
    "                count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ab515c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1 : 315220.75 bits: 18.266003\n"
     ]
    }
   ],
   "source": [
    "max = 0.01\n",
    "for i in max_values_layer1:\n",
    "    if max < i:\n",
    "        max = i\n",
    "print(\"layer1 :\", max, \"bits:\", np.log2(max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "10c4e9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer2 : 67630.68 bits: 16.045391\n"
     ]
    }
   ],
   "source": [
    "max = 0.01\n",
    "for i in max_values_layer2:\n",
    "    if max < i:\n",
    "        max = i\n",
    "print(\"layer2 :\", max, \"bits:\", np.log2(max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9228e822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer2 : 72450.16 bits: 16.144701\n"
     ]
    }
   ],
   "source": [
    "max = 0.01\n",
    "for i in max_values_layer3:\n",
    "    if max < i:\n",
    "        max = i\n",
    "print(\"layer2 :\", max, \"bits:\", np.log2(max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ee7b7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices where values do not match: []\n"
     ]
    }
   ],
   "source": [
    "mismatch_indices = np.where(input_vector != rounded_vector)[0]\n",
    "\n",
    "print(\"Indices where values do not match:\", mismatch_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1cd434",
   "metadata": {},
   "source": [
    "# Comparing the outputs of FPGA and pyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e073906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "data, targets = train_dataset[0]\n",
    "data.shape\n",
    "\n",
    "print(targets)\n",
    "\n",
    "# re-evaluating the model for generating outputs\n",
    "with torch.no_grad():\n",
    "    out, outputs, quants = model.forward_compute(data)\n",
    "    _, predicted = torch.max(out.value, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2df286fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pytorch outputs\n",
    "\n",
    "\n",
    "layers = [1, 2, 3, 4]\n",
    "bitwidth = 8\n",
    "\n",
    "for i in layers:\n",
    "    if(i==1):\n",
    "        #pytorch outputs\n",
    "        output_values = quants[0].value.numpy().flatten()\n",
    "        scale = quants[0].scale.item()\n",
    "        layer1_pytorch = float_to_int_n(output_values, scale, bitwidth, False)\n",
    "        \n",
    "    if(i==2):\n",
    "        #pytorch outputs\n",
    "        output_values = quants[1].value.numpy().flatten()\n",
    "        scale = quants[1].scale.item()\n",
    "        layer2_pytorch = float_to_int_n(output_values, scale, bitwidth, False)\n",
    "        \n",
    "    if(i==3):\n",
    "        #pytorch outputs\n",
    "        output_values = quants[2].value.numpy().flatten()\n",
    "        scale = quants[2].scale.item()\n",
    "        layer3_pytorch = float_to_int_n(output_values, scale, bitwidth, False)\n",
    "    \n",
    "    if(i==4):\n",
    "        #pytorch outputs\n",
    "        output_values = quants[3].value.numpy().flatten()\n",
    "        scale = quants[3].scale.item()\n",
    "        layer4_pytorch = float_to_int_n(output_values, scale, bitwidth, False)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3bb246d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FPGA simulation results\n",
    "\n",
    "layers = [0, 1, 2, 3]\n",
    "count = 0\n",
    "bitwidth = 8\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, qnn.QuantLinear):\n",
    "        if count in layers:\n",
    "            int_weights = module.int_weight().detach().cpu().numpy()\n",
    "            weight_scale = module.quant_weight_scale().detach().cpu().numpy() if hasattr(module, 'quant_weight_scale') else None\n",
    "            \n",
    "            if count == 0:\n",
    "                output_values = outputs[1].value.numpy().flatten()\n",
    "                scale = outputs[0].scale.item()*weight_scale\n",
    "                bitwidth = int(outputs[1].bit_width.item())\n",
    "                input_vector = float_to_int_n(output_values, scale, bitwidth, False) /(2**10)\n",
    "                out = input_vector.astype(int)\n",
    "                out[out<0] = 0\n",
    "                FPGA_outputlayer1 = out\n",
    "            \n",
    "            if count == 1:\n",
    "                input_vector = np.dot(int_weights, FPGA_outputlayer1) / (2**8)\n",
    "                out = input_vector.astype(int)\n",
    "                out[out<0] = 0\n",
    "                FPGA_outputlayer2 = out\n",
    "                \n",
    "            if count == 2:\n",
    "                input_vector = np.dot(int_weights, FPGA_outputlayer2) / (2**8)\n",
    "                out = input_vector.astype(int)\n",
    "                out[out<0] = 0\n",
    "                FPGA_outputlayer3 = out\n",
    "                \n",
    "            if count == 3:\n",
    "                input_vector = np.dot(int_weights, FPGA_outputlayer3) / (2**8)\n",
    "                out = input_vector.astype(int)\n",
    "                out[out<0] = 0\n",
    "                FPGA_outputlayer4 = out\n",
    "            \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2f85a480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pyTorch  Vivado\n",
      "0        0       0\n",
      "1        0       0\n",
      "2        0       0\n",
      "3      115     101\n",
      "4        0       0\n",
      "5      255     238\n",
      "6        0       0\n",
      "7        0       0\n",
      "8        0       0\n",
      "9        0       0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with the two arrays\n",
    "df = pd.DataFrame({'pyTorch': layer4_pytorch, 'Vivado': FPGA_outputlayer4})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Store the DataFrame in a text file\n",
    "df.to_csv('output.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ebc92c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pytorch_quantized_outputs(quants, layers, bitwidth=8):\n",
    "    outputs = {}\n",
    "    \n",
    "    for i in layers:\n",
    "        if i in range(1, 5):\n",
    "            # PyTorch outputs\n",
    "            output_values = quants[i-1].value.numpy().flatten()\n",
    "            scale = quants[i-1].scale.item()\n",
    "            outputs[f'layer{i}_pytorch'] = float_to_int_n(output_values, scale, bitwidth, False)\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a1b8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_fpga_simulation_outputs(model, outputs, bitwidth=8):\n",
    "    count = 0\n",
    "    results = {}\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, qnn.QuantLinear):\n",
    "                int_weights = module.int_weight().detach().cpu().numpy()\n",
    "                weight_scale = module.quant_weight_scale().detach().cpu().numpy() if hasattr(module, 'quant_weight_scale') else None\n",
    "\n",
    "                if count == 0:\n",
    "                    output_values = outputs[1].value.numpy().flatten()\n",
    "                    scale = outputs[0].scale.item() * weight_scale\n",
    "                    bitwidth = int(outputs[1].bit_width.item())\n",
    "                    input_vector = float_to_int_n(output_values, scale, bitwidth, False) / (2**10)\n",
    "                    out = input_vector.astype(int)\n",
    "                    out[out < 0] = 0\n",
    "                    results['FPGA_outputlayer1'] = out\n",
    "\n",
    "                if count == 1:\n",
    "                    input_vector = np.dot(int_weights, results['FPGA_outputlayer1']) / (2**8)\n",
    "                    out = input_vector.astype(int)\n",
    "                    out[out < 0] = 0\n",
    "                    results['FPGA_outputlayer2'] = out\n",
    "\n",
    "                if count == 2:\n",
    "                    input_vector = np.dot(int_weights, results['FPGA_outputlayer2']) / (2**8)\n",
    "                    out = input_vector.astype(int)\n",
    "                    out[out < 0] = 0\n",
    "                    results['FPGA_outputlayer3'] = out\n",
    "\n",
    "                if count == 3:\n",
    "                    input_vector = np.dot(int_weights, results['FPGA_outputlayer3']) / (2**8)\n",
    "                    out = input_vector.astype(int)\n",
    "                    out[out < 0] = 0\n",
    "                    results['FPGA_outputlayer4'] = out\n",
    "\n",
    "                count += 1\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01f9e525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer4_pytorch': array([255,   0,  28,   0,   0,   0,  17,   0,   0,   5], dtype=uint32)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = [4]\n",
    "\n",
    "pyOut = get_pytorch_quantized_outputs(quants, layers, bitwidth=8)\n",
    "pyOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0631dee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FPGA_outputlayer1': array([ 68,   0,   0,   0,   0,   0,   0,  70,  64,   0,   2,   0,   0,  24,  35,   0,   0,  32,   9,   0,   0,   0,   0,   0, 163,  81,   0, 120,   0, 163,   0,   4,  93,  70,  68,   0,   0,  25,   0,   0,   0,   0, 106,   0,  11,   0,   0,  44, 118,   0,   0,   0,   0,   0,   0,   0, 142,   0,   0, 137,   0,   0,   0,  53]),\n",
       " 'FPGA_outputlayer2': array([  0,   0,  33,  99,   0,   0, 118,  45,   0,  48,  14,   0,   0,   0,   0,   0,   0,  17,  17,   0,   0,  75,   9,  57,   0,  36,   0,   0,   0,   0,  68,  83,  65,  15,   0,  54,   0, 125,  39,   0,   0,   0,   0,   0,   7,   0,   0,   0,  40,   0,  10, 156,  21,  31,   0,   0,  36,  75,   9,  98,  11,  76,  28,  61]),\n",
       " 'FPGA_outputlayer3': array([  0,   3,   2,   0,   0,  76,   0,   0,  74,  60,   0, 111,   0,   0,   0,   1,   0,  26,  40, 108,  56,  35,   0,  89,   0,  74,  19,  10,  81,   0,  82,   0,   0,   0,  39,   0,  58,   7,   0,   7,  39,   0,   0,   0,  90,  13,  74,  64, 112,   0,  82,   0,   0,  46,  26,   0,   7,   0,  52,   0,   0,   0,   0,   0]),\n",
       " 'FPGA_outputlayer4': array([254,   0,  26,   0,   0,   0,  12,   0,   0,   5])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vivadoOut = get_fpga_simulation_outputs(model, outputs)\n",
    "vivadoOut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07796614",
   "metadata": {},
   "source": [
    "# Testing Entire Training + Testing Dataset Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "11f63e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_detailed_analysis(model, dataset, bitwidth=8):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    total_match = 0\n",
    "    total_samples = 0\n",
    "    layers = [4]\n",
    "    index = []\n",
    "    with torch.no_grad():\n",
    "        for data, targets in dataset:\n",
    "            out, outputs, quants = model.forward_compute(data)\n",
    "            _, predicted = torch.max(out.value, 1)\n",
    "            pyOut = get_pytorch_quantized_outputs(quants, layers, bitwidth)\n",
    "            vivadoOut = get_fpga_simulation_outputs(model, outputs)\n",
    "            py =  pyOut['layer4_pytorch']\n",
    "            viv = vivadoOut['FPGA_outputlayer4']          \n",
    "            \n",
    "            pyClass =  np.argmax(py)\n",
    "            vivClass = np.argmax(viv)\n",
    "            \n",
    "            if(pyClass == vivClass):\n",
    "                total_match = total_match + 1\n",
    "            else:\n",
    "                index.append(total_samples)\n",
    "                \n",
    "        \n",
    "            total_samples += 1\n",
    "\n",
    "    return index,total_match, total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4247eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "index, total, match = evaluate_model_with_detailed_analysis(model, train_dataset, bitwidth=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f74e3766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices where classes did not match: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "Total matches: 0\n",
      "Total samples: 32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model_with_detailed_analysis(model, dataset, indices, output_file, bitwidth=8):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    total_match = 0\n",
    "    total_samples = 0\n",
    "    layers = [4]\n",
    "    index = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with open(output_file, 'w') as f:\n",
    "            for i in indices:\n",
    "                data, targets = dataset[i]\n",
    "                out, outputs, quants = model.forward_compute(data)\n",
    "                _, predicted = torch.max(out.value, 1)\n",
    "                pyOut = get_pytorch_quantized_outputs(quants, layers, bitwidth)\n",
    "                vivadoOut = get_fpga_simulation_outputs(model, outputs)\n",
    "                py = pyOut['layer4_pytorch']\n",
    "                viv = vivadoOut['FPGA_outputlayer4']\n",
    "\n",
    "                pyClass = np.argmax(py)\n",
    "                vivClass = np.argmax(viv)\n",
    "\n",
    "                f.write(f\"Index: {i}\\n\")\n",
    "                f.write(f\"python: {py} vivado: {viv}\\n\\n\")\n",
    "                # f.write(f\"vivado: {viv}\\n\\n\")\n",
    "\n",
    "                if pyClass == vivClass:\n",
    "                    total_match += 1\n",
    "                else:\n",
    "                    index.append(total_samples)\n",
    "\n",
    "                total_samples += 1\n",
    "\n",
    "    return index, total_match, total_samples\n",
    "\n",
    "# Assuming train_dataset is the original dataset and indices is the array containing the given indices\n",
    "indices = [1674, 2734, 3065, 6418, 10852, 13234, 15741, 16446, 16658, 17728, 18966, 22270, 28632, 29609, 31347, 32573, 34785, 35382, 41897, 42986, 45925, 45930, 48166, 50091, 50239, 51544, 53578, 55078, 56842, 57302, 57982, 58822]\n",
    "\n",
    "output_file = \"output.txt\"\n",
    "\n",
    "index, total_match, total_samples = evaluate_model_with_detailed_analysis(model, train_dataset, indices, output_file)\n",
    "\n",
    "print(f\"Indices where classes did not match: {index}\")\n",
    "print(f\"Total matches: {total_match}\")\n",
    "print(f\"Total samples: {total_samples}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
