{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abb0d813",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1bbf3bc-f057-4b52-ab09-e5ea2d515279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import brevitas.nn as qnn\n",
    "\n",
    "# Define the transformation for dataset (if needed)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.quant_inp1 = qnn.QuantIdentity(bit_width=8, signed=False, return_quant_tensor=True)\n",
    "        self.fc1 = qnn.QuantLinear(784, 64, bias=False,\n",
    "                                   weight_bit_width=8,\n",
    "                                   bias_bit_width=8)\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=26, return_quant_tensor=True)\n",
    "        self.quant_inp2 = qnn.QuantIdentity(bit_width=8, signed=False, return_quant_tensor=True)\n",
    "        self.fc2 = qnn.QuantLinear(64, 64, bias=False,\n",
    "                                   weight_bit_width=8,\n",
    "                                   bias_bit_width=8)\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=22, return_quant_tensor=True)\n",
    "        self.quant_inp3 = qnn.QuantIdentity(bit_width=8, signed=False, return_quant_tensor=True)\n",
    "        self.fc3 = qnn.QuantLinear(64, 64, bias=False,\n",
    "                                   weight_bit_width=8,\n",
    "                                   bias_bit_width=8)\n",
    "        self.relu3 = qnn.QuantReLU(bit_width=22, return_quant_tensor=True)\n",
    "        self.quant_inp4 = qnn.QuantIdentity(bit_width=8, signed=False, return_quant_tensor=True)\n",
    "        self.fc4 = qnn.QuantLinear(64, 10, bias=False,\n",
    "                                   weight_bit_width=8,\n",
    "                                   bias_bit_width=8)\n",
    "        self.relu4 = qnn.QuantReLU(bit_width=22, return_quant_tensor=True)\n",
    "        self.quant_inp5 = qnn.QuantIdentity(bit_width=8, signed=False, return_quant_tensor=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)  # Flatten the input tensor\n",
    "        x = self.quant_inp1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.quant_inp2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.quant_inp3(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.quant_inp4(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.quant_inp5(x)\n",
    "        return x\n",
    "    \n",
    "    #new function to extract activations after each layer\n",
    "\n",
    "    def forward_compute(self, x):\n",
    "        x = torch.flatten(x, 1)  # Flatten the input tensor\n",
    "        y = []\n",
    "        z = []\n",
    "        x = self.quant_inp1(x)\n",
    "        y.append(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        y.append(x)\n",
    "        x = self.quant_inp2(x)\n",
    "        z.append(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        y.append(x)\n",
    "        x = self.quant_inp3(x)\n",
    "        z.append(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        y.append(x)\n",
    "        x = self.quant_inp4(x)\n",
    "        z.append(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        y.append(x)\n",
    "        x = self.quant_inp5(x)\n",
    "        z.append(x)\n",
    "        return x,y,z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79e74f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleNN()\n",
    "model.load_state_dict(torch.load('model_weights_quantized.pth', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c246e",
   "metadata": {},
   "source": [
    "### Some Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3232c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b21b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary(val, bitwidth):\n",
    "    if val < 0:\n",
    "        val = (1 << bitwidth) + val\n",
    "    return f\"{val:0{bitwidth}b}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f76ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_int_n(values, scale, bit_width, signed=True):\n",
    "    # Calculate the scaled values\n",
    "    scaled_values = values / scale\n",
    "    scaled_values = np.round(scaled_values)\n",
    "    \n",
    "    if(signed == True):\n",
    "        # Determine the range for n-bit signed integers\n",
    "        int_n_min = -2**(bit_width - 1)\n",
    "        int_n_max = 2**(bit_width - 1) - 1\n",
    "        \n",
    "        # Clip the values to fit within n-bit signed integer range\n",
    "        scaled_values = np.clip(scaled_values, int_n_min, int_n_max)\n",
    "        \n",
    "        # Convert to n-bit integers by truncating to fit in n-bits\n",
    "        int_n_values = scaled_values.astype(np.int32)  # Initially convert to 32-bit integers\n",
    "        \n",
    "        # Manually handle the n-bit integer range\n",
    "        int_n_values = int_n_values & ((1 << bit_width) - 1)  # Mask to n bits\n",
    "        \n",
    "        # Handle sign extension for negative values\n",
    "        sign_bit = 1 << (bit_width - 1)\n",
    "        int_n_values = np.where(int_n_values & sign_bit, int_n_values | ~((1 << bit_width) - 1), int_n_values)\n",
    "        \n",
    "    else:\n",
    "        # Determine the range for n-bit unsigned integers\n",
    "        int_n_min = 0\n",
    "        int_n_max = 2**bit_width - 1\n",
    "\n",
    "        # Clip the values to fit within n-bit unsigned integer range\n",
    "        scaled_values = np.clip(scaled_values, int_n_min, int_n_max)\n",
    "\n",
    "        # Convert to n-bit integers by truncating to fit in n-bits\n",
    "        int_n_values = scaled_values.astype(np.uint32)  # Initially convert to 32-bit unsigned integers\n",
    "\n",
    "        # Manually handle the n-bit integer range\n",
    "        int_n_values = int_n_values & ((1 << bit_width) - 1)  # Mask to n bits\n",
    "        \n",
    "    \n",
    "    return int_n_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d1d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pytorch_quantized_outputs(quants, layers, bitwidth=8):\n",
    "    outputs = {}\n",
    "    \n",
    "    for i in layers:\n",
    "        if i in range(1, 5):\n",
    "            # PyTorch outputs\n",
    "            output_values = quants[i-1].value.numpy().flatten()\n",
    "            scale = quants[i-1].scale.item()\n",
    "            outputs[f'layer{i}_pytorch'] = float_to_int_n(output_values, scale, bitwidth, False)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41492e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_fpga_simulation_outputs(model, outputs, bitwidth=8):\n",
    "    count = 0\n",
    "    results = {}\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, qnn.QuantLinear):\n",
    "                int_weights = module.int_weight().detach().cpu().numpy()\n",
    "                weight_scale = module.quant_weight_scale().detach().cpu().numpy() if hasattr(module, 'quant_weight_scale') else None\n",
    "\n",
    "                if count == 0:\n",
    "                    output_values = outputs[1].value.numpy().flatten()\n",
    "                    scale = outputs[0].scale.item() * weight_scale\n",
    "                    bitwidth = int(outputs[1].bit_width.item())\n",
    "                    input_vector = float_to_int_n(output_values, scale, bitwidth, False) / (2**10)\n",
    "                    out = input_vector.astype(int)\n",
    "                    out[out < 0] = 0\n",
    "                    results['FPGA_outputlayer1'] = out\n",
    "\n",
    "                if count == 1:\n",
    "                    input_vector = np.dot(int_weights, results['FPGA_outputlayer1']) / (2**8)\n",
    "                    out = input_vector.astype(int)\n",
    "                    out[out < 0] = 0\n",
    "                    results['FPGA_outputlayer2'] = out\n",
    "\n",
    "                if count == 2:\n",
    "                    input_vector = np.dot(int_weights, results['FPGA_outputlayer2']) / (2**8)\n",
    "                    out = input_vector.astype(int)\n",
    "                    out[out < 0] = 0\n",
    "                    results['FPGA_outputlayer3'] = out\n",
    "\n",
    "                if count == 3:\n",
    "                    input_vector = np.dot(int_weights, results['FPGA_outputlayer3']) / (2**8)\n",
    "                    out = input_vector.astype(int)\n",
    "                    out[out < 0] = 0\n",
    "                    results['FPGA_outputlayer4'] = out\n",
    "\n",
    "                count += 1\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af9df348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with open(\"input_data.txt\", \"w\") as f:\n",
    "    for i in range(2):\n",
    "        data, targets = train_dataset[i]\n",
    "\n",
    "        # Re-evaluating the model for generating outputs\n",
    "        with torch.no_grad():\n",
    "            out, outputs, quants = model.forward_compute(data)\n",
    "            _, predicted = torch.max(out.value, 1)\n",
    "        \n",
    "        # Assuming the output is the same as mentioned\n",
    "        output_values = outputs[0].value.numpy().flatten()\n",
    "        scale = outputs[0].scale.item()\n",
    "        bitwidth = 9\n",
    "\n",
    "        # Convert the floating point values to 8-bit signed integers\n",
    "        input_vector = float_to_int_n(output_values, scale, bitwidth, False)\n",
    "\n",
    "        # Convert to binary and write to file\n",
    "        input_vector_verilog = [to_binary(val, bitwidth) for val in input_vector]\n",
    "        \n",
    "        for bin_value in input_vector_verilog:\n",
    "            f.write(f\"{bin_value}\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70986c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_detailed_analysis(model, dataset, bitwidth=8):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    total_match = 0\n",
    "    total_samples = 0\n",
    "    layers = [4]\n",
    "    index = []\n",
    "    with torch.no_grad():\n",
    "        for data, targets in dataset:\n",
    "            out, outputs, quants = model.forward_compute(data)\n",
    "            _, predicted = torch.max(out.value, 1)\n",
    "            pyOut = get_pytorch_quantized_outputs(quants, layers, bitwidth)\n",
    "            vivadoOut = get_fpga_simulation_outputs(model, outputs)\n",
    "            py =  pyOut['layer4_pytorch']\n",
    "            viv = vivadoOut['FPGA_outputlayer4']          \n",
    "            \n",
    "            pyClass =  np.argmax(py)\n",
    "            vivClass = np.argmax(viv)\n",
    "            \n",
    "            if(pyClass == vivClass):\n",
    "                total_match = total_match + 1\n",
    "            else:\n",
    "                index.append(total_samples)\n",
    "                \n",
    "        \n",
    "            total_samples += 1\n",
    "\n",
    "    return index,total_match, total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb7bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index, total, match = evaluate_model_with_detailed_analysis(model, train_dataset, bitwidth=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbf3659",
   "metadata": {},
   "source": [
    "#### Storing Mismatch Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f478b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model_with_detailed_analysis(model, dataset, indices, output_file, bitwidth=8):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    total_match = 0\n",
    "    total_samples = 0\n",
    "    layers = [4]\n",
    "    index = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with open(output_file, 'w') as f:\n",
    "            for i in indices:\n",
    "                data, targets = dataset[i]\n",
    "                out, outputs, quants = model.forward_compute(data)\n",
    "                _, predicted = torch.max(out.value, 1)\n",
    "                pyOut = get_pytorch_quantized_outputs(quants, layers, bitwidth)\n",
    "                vivadoOut = get_fpga_simulation_outputs(model, outputs)\n",
    "                py = pyOut['layer4_pytorch']\n",
    "                viv = vivadoOut['FPGA_outputlayer4']\n",
    "\n",
    "                pyClass = np.argmax(py)\n",
    "                vivClass = np.argmax(viv)\n",
    "\n",
    "                f.write(f\"Index: {i}\\n\")\n",
    "                f.write(f\"python: {py} vivado: {viv}\\n\\n\")\n",
    "                # f.write(f\"vivado: {viv}\\n\\n\")\n",
    "\n",
    "                if pyClass == vivClass:\n",
    "                    total_match += 1\n",
    "                else:\n",
    "                    index.append(total_samples)\n",
    "\n",
    "                total_samples += 1\n",
    "\n",
    "    return index, total_match, total_samples\n",
    "\n",
    "# Assuming train_dataset is the original dataset and indices is the array containing the given indices\n",
    "indices = [1674, 2734, 3065, 6418, 10852, 13234, 15741, 16446, 16658, 17728, 18966, 22270, 28632, 29609, 31347, 32573, 34785, 35382, 41897, 42986, 45925, 45930, 48166, 50091, 50239, 51544, 53578, 55078, 56842, 57302, 57982, 58822]\n",
    "\n",
    "output_file = \"output.txt\"\n",
    "\n",
    "index, total_match, total_samples = evaluate_model_with_detailed_analysis(model, train_dataset, indices, output_file)\n",
    "\n",
    "print(f\"Indices where classes did not match: {index}\")\n",
    "print(f\"Total matches: {total_match}\")\n",
    "print(f\"Total samples: {total_samples}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
