{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bd261ac",
   "metadata": {},
   "source": [
    "Model Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b1e5b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.1844730279974338\n",
      "Epoch 2/10, Loss: 1.0443896804410002\n",
      "Epoch 3/10, Loss: 1.0076571052897967\n",
      "Epoch 4/10, Loss: 0.9869441156829598\n",
      "Epoch 5/10, Loss: 0.9734681997217858\n",
      "Epoch 6/10, Loss: 0.962691763062467\n",
      "Epoch 7/10, Loss: 0.9536674909754348\n",
      "Epoch 8/10, Loss: 0.9481916370740069\n",
      "Epoch 9/10, Loss: 0.9414239671311653\n",
      "Epoch 10/10, Loss: 0.9403496995917769\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import brevitas.nn as qnn\n",
    "\n",
    "# Define the transformation for dataset (if needed)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.quant_inp1 = qnn.QuantIdentity(bit_width=8, signed=False, return_quant_tensor=True)\n",
    "        self.fc1 = qnn.QuantLinear(784, 64, bias=False,\n",
    "                                   weight_bit_width=8,\n",
    "                                   bias_bit_width=8)\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=26, return_quant_tensor=True)\n",
    "        self.quant_inp2 = qnn.QuantIdentity(bit_width=8, signed=False, return_quant_tensor=True)\n",
    "        self.fc2 = qnn.QuantLinear(64, 64, bias=False,\n",
    "                                   weight_bit_width=8,\n",
    "                                   bias_bit_width=8)\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=22, return_quant_tensor=True)\n",
    "        self.quant_inp3 = qnn.QuantIdentity(bit_width=8, signed=False, return_quant_tensor=True)\n",
    "        self.fc3 = qnn.QuantLinear(64, 64, bias=False,\n",
    "                                   weight_bit_width=8,\n",
    "                                   bias_bit_width=8)\n",
    "        self.relu3 = qnn.QuantReLU(bit_width=22, return_quant_tensor=True)\n",
    "        self.quant_inp4 = qnn.QuantIdentity(bit_width=8, signed=False, return_quant_tensor=True)\n",
    "        self.fc4 = qnn.QuantLinear(64, 10, bias=False,\n",
    "                                   weight_bit_width=8,\n",
    "                                   bias_bit_width=8)\n",
    "        self.relu4 = qnn.QuantReLU(bit_width=22, return_quant_tensor=True)\n",
    "        self.quant_inp5 = qnn.QuantIdentity(bit_width=8, signed=False, return_quant_tensor=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)  # Flatten the input tensor\n",
    "        x = self.quant_inp1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.quant_inp2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.quant_inp3(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.quant_inp4(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.quant_inp5(x)\n",
    "        return x    \n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleNN()\n",
    "\n",
    "# Define the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(data)  # Forward pass\n",
    "        loss = criterion(outputs, targets)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        running_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b96a5310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 59.49%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader:\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.value, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "print(f\"Accuracy on test set: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dda08fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights_quantized.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ceea7d",
   "metadata": {},
   "source": [
    "# FP32 Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e5dfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.4320\n",
      "Epoch [2/5], Loss: 0.2027\n",
      "Epoch [3/5], Loss: 0.1545\n",
      "Epoch [4/5], Loss: 0.1295\n",
      "Epoch [5/5], Loss: 0.1135\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the transformation for the dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the SimpleNN model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)  # Flatten the input tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function and optimizer\n",
    "model_fp32 = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_fp32.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model_fp32.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_fp32(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Save the trained model weights\n",
    "torch.save(model_fp32.state_dict(), 'model_fp32.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd35305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import brevitas.nn as qnn\n",
    "\n",
    "# Define the SimpleNN model with Brevitas quantization\n",
    "class SimpleNNQuant(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNNQuant, self).__init__()\n",
    "        self.quant_inp1 = qnn.QuantIdentity(bit_width=8, signed=False, return_quant_tensor=True)\n",
    "        self.fc1 = qnn.QuantLinear(784, 64, bias=False, weight_bit_width=8)\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=26, return_quant_tensor=True)\n",
    "        self.quant_inp2 = qnn.QuantIdentity(bit_width=8, signed=False, return_quant_tensor=True)\n",
    "        self.fc2 = qnn.QuantLinear(64, 64, bias=False, weight_bit_width=8)\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=22, return_quant_tensor=True)\n",
    "        self.quant_inp3 = qnn.QuantIdentity(bit_width=8, signed=False, return_quant_tensor=True)\n",
    "        self.fc3 = qnn.QuantLinear(64, 64, bias=False, weight_bit_width=8)\n",
    "        self.relu3 = qnn.QuantReLU(bit_width=22, return_quant_tensor=True)\n",
    "        self.quant_inp4 = qnn.QuantIdentity(bit_width=8, signed=False, return_quant_tensor=True)\n",
    "        self.fc4 = qnn.QuantLinear(64, 10, bias=False, weight_bit_width=8)\n",
    "        self.relu4 = qnn.QuantReLU(bit_width=22, return_quant_tensor=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)  # Flatten the input tensor\n",
    "        x = self.quant_inp1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.quant_inp2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.quant_inp3(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.quant_inp4(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        return x\n",
    "\n",
    "# Load the FP32 model weights\n",
    "model_fp32_weights = torch.load('model_fp32.pth')\n",
    "\n",
    "# Initialize the quantized model\n",
    "model_quant = SimpleNNQuant()\n",
    "\n",
    "# Copy the weights from FP32 model to quantized model\n",
    "model_quant.fc1.weight.data = model_fp32_weights['fc1.weight'].clone().detach()\n",
    "model_quant.fc2.weight.data = model_fp32_weights['fc2.weight'].clone().detach()\n",
    "model_quant.fc3.weight.data = model_fp32_weights['fc3.weight'].clone().detach()\n",
    "model_quant.fc4.weight.data = model_fp32_weights['fc4.weight'].clone().detach()\n",
    "\n",
    "# Since bias is not used in quantized model, we do not copy biases\n",
    "\n",
    "# The quantized model is now ready for further use\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
